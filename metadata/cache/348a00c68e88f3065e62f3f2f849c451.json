{
  "cache_date": "2025-09-01T08:52:40.801836",
  "data": "\n<p align=\"center\"> <img src=\"logo_marvel.png\" height=\"110\" alt=\"MARVEL\" /> </p>\n\n# Metagenomic Analysis and Retrieval of Viral Elements\n\nMARVEL is a tool for recovery of draft phage genomes from whole community shotgun metagenomic sequencing data.  \n\nMain script:\n   * **marvel_bins.py** - Machine learning prediction of phage bins\n  \nAuxiliary script:\n   * **generate_bins_from_reads.py** - Generates metagenomic bins, given Illumina sequencing reads\n\n## Reference and citation\n\nA manuscript describing MARVEL was published in [Frontiers in Genetics](https://www.frontiersin.org/articles/10.3389/fgene.2018.00304/full).  \nIf you find MARVEL useful in your research, please cite:  \n*Amgarten DE, Braga LP, Da Silva AM, Setubal JC. MARVEL, a Tool for Prediction of Bacteriophage Sequences in Metagenomic Bins. Frontiers in Genetics. 2018;9:304.*\n\n## Change Log\n#### MARVEL Version 0.2 - April 2019\n  * Fix for Prokka recent changes in its out files. All Prokka versions should work just fine.\n  * Bins containig contigs which are too short (<2000bp) will not be considered in downstream analyses and a warning will be printed.\n  * Minor improvements.\n  * Bugs ou others suggestions for improvement, just let me know: deyvid.amgarten@usp.br\n\n## New Viral Groups\n\nWe are working to train models to new viral groups. Let us know if a particular group would be helpful in your research.\n\n## Dependencies\n\nAll scripts from this project were coded in [Python 3](https://www.python.org/). So, first of all, make sure you have it installed and updated.  \nMARVEL's main scrip (marvel_bins.py) requires Prokka and HMMER tools as dependencies. By installing Prokka and its dependencies, you will usually  install HMMER tools automatically.  \n\n* [Prokka](https://github.com/tseemann/prokka) - Rapid Prokaryotic genome annotation.\n* [HMMER Tools](http://www.hmmer.org/) - Biosequence analysis using profile hidden Markov models\n\nThese Python libraries are required:\n\n* [Numpy](http://www.numpy.org/), [Scipy](https://www.scipy.org/) - Efficiently handling arrays and scientific computing\n* [Biopython](http://biopython.org/) - Handling biological sequences and records\n* [Scikit-learn](http://scikit-learn.org/stable/) - Machine learning\n\nTo install these Python libraries, just type: \n```\npip3 install -U numpy scipy biopython scikit-learn\n```\n\n* If you use conda you can use the `environment.yml` file within this project to install all dependencies.\n\n```\n$ conda env create -n marvel -f=environment.yml\n$ conda activate marvel\n(marvel)$ python marvel_bins.py -h\n```\n\n## Installing\n\nGetting MARVEL ready to run is as simple as cloning this Github project or dowload and extract it to a directory inside your computer:\n\n```\ngit clone https://github.com/LaboratorioBioinformatica/MARVEL\n```\n\n## Quick start\n\nInside the directory where MARVEL was extracted (or cloned), you will need to download and set the models. \nThis is required only once and it is simple. Just run:\n```\npython3 download_and_set_models.py\n```\nAll set!  \nNow, to run MARVEL type:\n```\npython3 marvel_bins.py -i input_directory -t num_threads\n```\n\nChange 'input_directory' to the folder where bins are stored in fasta format and 'num_threads' to the number of CPU cores to be used. Several threads should be used to speed up prokka and hmm searches.  \nResults will be stored in the 'Results' folder inside the input directory.  \nObs: You need to execute the scripts from the directory where MARVEL was extracted, i.e., MARVEL's root folder. \n\n## Running the example datasets\n\nWe provide a folder with example datasets containing mocking bins of RefSeq viral and bacterial genomes.  \nTo try these examples, run:\n\n```\npython3 marvel_bins.py -i example_data/bins_8k_refseq -t 12\n```\n\n## Additional scripts\n\nMARVEL's main script receives metagenomic bins as input. However, we additionally provide a simple scrip which receives\nmetagenomic reads (Illumina sequencing) and generates bins.\n[metaSpades](http://bioinf.spbau.ru/spades), [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml) and [Metabat2](https://bitbucket.org/berkeleylab/metabat) are used for assembling, mapping and binning, respectively.  \n\nIn case you want to generate the bins by yourself, we recommend two special parameters in the binning process: -m 1500 -s 10000. These parameters tell metabat to generate bins with contigs of at least 1500 bp and with a minimum total size of 10 kbp, which makes more sense when one is trying to retrieve viral genomes.  \nWe can't stress enough that there are several tools for assembly and binning, which should be well-chosen according to\nthe researcher's purposes. Our intention here is to facilitate the use of our tool.  \n\n```\npython3 generate_bins_from_reads.py -1 reads-R1.fastq -2 reads-R2.fastq -t num_threads\n```\n## Suggested workflow for improved genome recovery\n\nMARVEL is indicated to metagenomic studies where whole community DNA sequencing reads are available. The process of binning with Metabat2 will work better if you have several samples from a same environment (time-series samples for example), so Metabat2 can assess contigs' abundancy correlation among samples and create better bins. Here we suggest a workflow of analyses to generate draft phage genomes from time-series samples raw sequencing reads.\n\n1. Assembly raw reads for each sample individually with [metaSpades](http://bioinf.spbau.ru/spades).\n\n2. Join contigs in a single multi-FASTA and use dedupe from [BBMap](https://github.com/BioInfoTools/BBMap) tools to remove duplicated contigs.\n\n3. Use resulting deduplicated contigs as reference for mapping samples' reads individually with [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml).\n\n3. Generate bins with metabat2 using BAM files obtained in step 3. At this point, it is important to set these specific parameters in metabat2: -m 1500 -s 10000. You should use these parameters, so metabat2 can generate bins with phage genomes characteristics.\n\n4. Run MARVEL giving bins folder as input. Bins predicted as phage will be in the folder: results/phage_genomes/.\n\nFor improved draft genomes, we suggested the following additional steps:\n\n6. Merge contigs with overlapping ends with [Phrap](http://www.phrap.org/phredphrapconsed.html) for each individual bin predicted by MARVEL as phage.\n\n7. Further validate predicted bins by assessing bacterial/archaeal genes with [CheckM](https://github.com/Ecogenomics/CheckM/wiki) and predicting tail/capsid proteins with [VIRALpro](http://scratch.proteomics.ics.uci.edu/explanation.html#VIRALpro).\n\nIf you have any question, please contact us. We will be glad to help with your analyses.  \ndeyvid.amgarten@usp.br\n\n## Simulated RefSeq datasets for training and testing\n\nAll the simulated datasets used for training and testing the Random Forest classifier, as well as predicted bins from composting samples were made available through this link:\n\n[Browse and Download datasets](http://projetos.lbi.iq.usp.br/metazoo/deyvid/datasets/) \n\n## Author\n[Deyvid Amgarten](https://sites.google.com/view/deyvid/english)  \nThis tool was developed as part of my PhD thesis by the [Bioinformatics Graduate Program](https://www.ime.usp.br/en/bioinformatics/graduate) from the University of Sao Paulo, Brazil.\n\n## Setulab\nOur group is interested in studying:\n\n* Classical machine learning techniques to create predictors, which are being applied in genomics and metagenomics;\n* Deep Learning techniques applied to more broad problems, as for instance prediction of complex biological attributes in viruses and bacteria.\n\nPlease visit [Setulab's page](http://lbi.usp.br/learning/) for more information.\n\n\n## License\n\nThis project is licensed under GNU license. Codes here may be used for any purposed or modified.  \nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n"
}