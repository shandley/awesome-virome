{
  "cache_date": "2025-03-17T02:07:40.921048",
  "data": "# Identification of bacteriophage genome sequences with representation learning\n\n![Pipeline_new](https://github.com/Celestial-Bai/INHERIT/blob/master/pipeline.jpeg)This repository includes the implementation of \"Identification of bacteriophage genome sequences with representation learning\". Our method is developed for identifying phages from the metagenome sequences, and the past methods can be categorized into two types: database-based methods and alignment-free methods.  INHERIT integrated those two types of methods by pre-train-fine-tune paradigm.  Our experiments show that INHERIT achieves better performance than the database-based approaches and the alignment-free methods.\n\nWe are still developing this package and we will also try to make some improvements of it, so feel free to report to us if there are any issues occurred. INHERIT is a model based on [DNABERT](https://github.com/jerryji1993/DNABERT) , an extension of [Huggingface's Transformers](https://github.com/huggingface/transformers). We will still update INHERIT for exploring better performance and more convenient utilization.\n\nOur package includes the resources of: \n\n- The source code of INHERIT\n- The pre-trained models and the fine-tuned model (INHERIT)\n \nPlease cite our paper if you want to include or use INHERIT in your research: [INHERIT](https://academic.oup.com/bioinformatics/article/38/18/4264/6654586)\n \n## Installation\n\n```\ngit clone https://github.com/Celestial-Bai/INHERIT.git\ncd INHERIT\npip install -r dependencies.txt\n```\n\n\n\n## Environment and requirements\n\n\nWe use NVIDIA A100 GPUs to train INHERIT with CUDA 11.4.  We also tested our codes on other GPUs, like V100, and they can run smoothly.\n\nBefore you start to use INHERIT, you should download those packages before you run our codes. We used the Transformers 4.7.0 on the **fine-tuning** process (we prove the results are the same) It should be noted that we will test to use Huggingface's  Transformers directly instead of using the source code in the future for better usage :)\n\n```\n##### All #####\nargparse\nnumpy\ntorch\ntqdm\n\n##### Transformers #####\n##### We will try to use the whole package in the future #####\nPillow\nblack==21.4b0\ncookiecutter==1.7.2\ndataclasses\ndatasets\ndeepspeed>=0.3.16\ndocutils==0.16.0\nfairscale>0.3\nfaiss-cpu\nfastapi\nfilelock\nflake8>=3.8.3\nflax>=0.3.2\nfugashi>=1.0\nhuggingface-hub==0.0.8\nimportlib_metadata\nipadic>=1.0.0,<2.0\nisort>=5.5.4\njax>=0.2.8\njieba\nkeras2onnx\nnltk\nonnxconverter-common\nonnxruntime-tools>=1.4.2\nonnxruntime>=1.4.0\npackaging\nparameterized\nprotobuf\npsutil\npydantic\npytest\npytest-sugar\npytest-xdist\nrecommonmark\nregex!=2019.12.17\nrequests\nrouge-score\nsacrebleu>=1.4.12\nsacremoses\nsagemaker>=2.31.0\nscikit-learn==0.24.1\nsentencepiece==0.1.91\nsoundfile\nsphinx-copybutton\nsphinx-markdown-tables\nsphinx-rtd-theme==0.4.3\nsphinx==3.2.1\nsphinxext-opengraph==0.4.1\nstarlette\ntensorflow-cpu>=2.3\ntensorflow>=2.3\ntimeout-decorator\ntokenizers>=0.10.1,<0.11\ntorchaudio\nunidic>=1.0.2\nunidic_lite>=1.0.7\nuvicorn\n```\n\n\n\n## Predict \n\nThe pre-trained models are the important parts of INHERIT.  **Please first download those two pre-trained models before you use INHERIT**. For the checkpoints of the pre-trained models we used, you can find in: [Bacteria pre-trained model download link](https://drive.google.com/drive/folders/1d0ubDne87j5rf5K6DYKSOKxnw_eGV-Dr?usp=sharing) and [Phage pre-trained model download link](https://drive.google.com/drive/folders/17oyt613Hr4984SX7IX72fVP3zPrGADvB?usp=sharing)\n\nYou can simply used\uff1a\n\n```\ncd INHERIT\npython3 IHT_predict.py --sequence test.fasta --withpretrain True --model INHERIT.pt --out test_out.txt\n```\n\n\n## Pre-trained models\n\nTo pre-train the models, we used the original DNABERT codes to pre-train. Therefore, please refer the guides on [DNABERT 2.2 Model Training](https://github.com/jerryji1993/DNABERT#2-pre-train-skip-this-section-if-you-fine-tune-on-pre-trained-models) to get a new pre-trained model if you are interested.  We welcome everyone to build any new pre-trained models to improve the performance of INHERIT. We also post the commands below: \n\n```\ncd examples\n\nexport KMER=6\nexport TRAIN_FILE=sample_data/pre/6_3k.txt\nexport TEST_FILE=sample_data/pre/6_3k.txt\nexport SOURCE=PATH_TO_DNABERT_REPO\nexport OUTPUT_PATH=output$KMER\n\npython run_pretrain.py \\\n    --output_dir $OUTPUT_PATH \\\n    --model_type=dna \\\n    --tokenizer_name=dna$KMER \\\n    --config_name=$SOURCE/src/transformers/dnabert-config/bert-config-$KMER/config.json \\\n    --do_train \\\n    --train_data_file=$TRAIN_FILE \\\n    --do_eval \\\n    --eval_data_file=$TEST_FILE \\\n    --mlm \\\n    --gradient_accumulation_steps 25 \\\n    --per_gpu_train_batch_size 10 \\\n    --per_gpu_eval_batch_size 6 \\\n    --save_steps 500 \\\n    --save_total_limit 20 \\\n    --max_steps 200000 \\\n    --evaluate_during_training \\\n    --logging_steps 500 \\\n    --line_by_line \\\n    --learning_rate 4e-4 \\\n    --block_size 512 \\\n    --adam_epsilon 1e-6 \\\n    --weight_decay 0.01 \\\n    --beta1 0.9 \\\n    --beta2 0.98 \\\n    --mlm_probability 0.025 \\\n    --warmup_steps 10000 \\\n    --overwrite_output_dir \\\n    --n_process 24\n```\n\n\n\n## Fine-tuning\n\nSince we have too many hyperparameters, we used \"IHT_config.py\" to record the default hyperparameters we used. You can change them depending on the different scenario, and you can simply used:\n\n```\ncd INHERIT\npython3 IHT_training.py\n```\n\nto train INHERIT.\nWe also have prepared the code of DNABERT if you want to explore the difference between DNABERT and INHERIT. You can also simply used:\n\n```\ncd INHERIT\npython3 DNABERT_training.py\n```\n\nto train DNABERT. Both of their training process are straightforward and easy. You do not need to add any other commands.\n\n \nYou can download INHERIT on: [Fine-tuned INHERIT download link](https://drive.google.com/file/d/169BaPS4BjK-cmnfabpyjNVTjFtyzhbyC/view?usp=sharing)\n\n## Reference\n\n- [DNABERT](https://github.com/jerryji1993/DNABERT) \n- [Huggingface's Transformers](https://github.com/huggingface/transformers)\n- [Seeker](https://github.com/gussow/seeker)\n- [VIBRANT](https://github.com/AnantharamanLab/VIBRANT)\n\n\n\n"
}