{
  "cache_date": "2025-03-17T02:19:29.504335",
  "data": "# VIDHOP\nVIDHOP is a virus host predicting tool. Its able to [predict](#examples-for-vidhop-predict:) influenza A virus, rabies lyssavirus and rotavirus A. Furthermore the user can [train](#train-and-use-your-own-model:) its own models for other viruses and use them with VIDHOP. This is the tool descripted in the [original paper](https://doi.org/10.1093/bioinformatics/btaa705), for the supplementary and training data look [here](https://www.doi.org/10.17605/OSF.IO/UXT7N).\n\n## Install: ##\n\nWe recommend to use linux and miniconda for the enviroment management\n\n1.  [Download and install Conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n2.  Download [the enviroment yml file](https://github.com/flomock/vidhop/blob/master/vidhop.yml)\n    \n3.  Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use VIDHOP.\n\n    ```bash\n    conda env create -f vidhop.yml\n    ```\n4.  Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use vidop.\n\n    ```bash\n    conda activate vidhop\n    ```\n\n## How to use:\n\nVIDHOP has three different commands each with its own parameter set.\n\n**make_dataset:** create the data structure needed for training\n\n**training:** train a model on your training files generated with make_dataset\n\n**predict:** predict the host of the viral sequence given \n\nUse ```vidhop --help``` to see this summary of all three methods.\n\nLikely you'll mainly use predict, see below various examples: \n\n#### examples for vidhop predict:\n\n\n```\nvidhop predict -i /home/user/fasta/influenza.fna -v influ\n```\n  present only hosts which reach a threshold of 0.2\n```\nvidhop predict -i /home/user/fasta/influenza.fna -v influ -t 0.2\n```\n  if you want the output in a file\n```\nvidhop predict -i /home/user/fasta/influenza.fna -v influ -o /home/user/vidhop_result.txt\n```\n  use multiple fasta-files in directory\n```\nvidhop predict -i /home/user/fasta/ -v rabies\n```\n  use multiple fasta-files in directory and only present top 3 host predictions per sequence\n```\nvidhop predict -i /home/user/fasta/ -v rabies -n_hosts\n```\nUse your own trained models generated with [vidhop training](#Train-and-use-your-own-model:). \nYou need to specify the path to the .model file you want to use. It's located in the output directory of vidhop \ntraining. You can choose between the model with the lowest loss and the model with the highest accuracy while training.\n```\nvidhop predict -v /home/user/out_training/model_best_acc_testname.model -i /home/user/fasta/my_favourite_virus.fna\n```\n\n**Options:**\n\ncommand | what it does\n  ------------- | -------------\n-i, --input     |either raw sequences or path to fasta file or directory with multiple files.  [required]\n-v, --virus     |select virus species (influ, rabies, rota) [required]\n-o, --outpath   |path where to save the output\n-n, --n_hosts   |show only -n most likely hosts\n-t, --thresh    |show only hosts with higher likeliness then --thresh\n--auto_filter   |automatically filters output to present most relevant host\n--help          |show this message and exit.\n--version       |show version number from vidhop\n\n**Example output and what it means:**\n```\nper autofilter selected host of interest\nAnas crecca: 0.3680844306945801\nAnser fabalis: 0.22696886956691742\n\nall hosts\nAnas crecca: 0.3680844306945801\nAnser fabalis: 0.22696886956691742\nCairina moschata: 0.14039446413516998\nAnas acuta: 0.0806809589266777\nAnas platyrhynchos: 0.0766301080584526\nAnas clypeata: 0.06576793640851974\nAnas rubripes: 0.01164703257381916\nMareca strepera: 0.009029239416122437\nGallus gallus: 0.005793101154267788\nAnas discors: 0.0032478864304721355\nTadorna ferruginea: 0.0021137858275324106\nStruthio camelus: 0.0017334294971078634\nChroicocephalus ridibundus: 0.001212031696923077\nAnas carolinensis: 0.0011747170938178897\nCygnus columbianus: 0.0009851191425696015\n```\n\nThe output is a list of potential hosts sorted by probability. For example an value of 0.368 represents a likelihood of 36.8% that this is the correct host, according to VIDHOP.\nThe autofilter selects hosts the most promissing candidates.\n\n## Train and use your own model:\n\n**If you like to skip ahead see [toy-example](#toy-example).**\n\nTrain your own model for other viruses than the provided ones (influenza A virus, rabies lyssavirus and rotavirus A) is simple.\nThe workflow consists of three steps:\n\n1.  [vidhop make_dataset](#vidhop-make_dataset)\n\n2.  [vidhop training](#vidhop-training)\n\n3.  [vidhop predict](#examples-for-vidhop-predict)\n\n \n\n#### vidhop make_dataset\n\nTo generate the data sets needed for training you need to provide two input files.\n\n1.  A sequence file containing in each line a DNA sequence.\n\n2.  A host file containing the name of the host corresponding to the DNA sequence at identical line number in the sequence file.\n\n**example input**\n\nsequences.txt | hosts.txt\n  ------------- | -------------\nAAATTT | human\nCGTATA | swine\nCGTATT | swine\n\n**examples for vidhop make_dataset:**\n\nExample:\nset input and output parameter\n```\nvidhop make_dataset -x /home/user/input/seq.txt -y /home/user/input/host.txt -o /home/user/trainingdata/\n```\n\nchange the validation set size and provide datastructure for repeated undersampling\n```\nvidhop make_dataset -x /home/user/input/seq.txt -y /home/user/input/host.txt -v 0.1 -r\n```\n\ncommand | what it does\n  ------------- | -------------\n-x, --sequences     |Path to the file containing sequence list  [required].\n-y, --hosts     |Path to the file containing corresponding host list [required].\n-o, --outpath   |Path where to save the output.\n-n, --n_hosts   |Show only -n most likely hosts.\n-v, --val_split_size    |Select the portion of the data which is used for the validation set.\n-t, --test_split_size   |Select the portion of the data which is used for the test set.\n-r, --repeated_undersampling       |Generate training files needed for reapeted undersampling while training.\n--help          |Show this message and exit.\n\n#### vidhop training\n\nThe training of a model is done by providing the output directory of vidhop make_dataset as the input of vidhop training.\nThe user can specify various parameter which change the architecture, training duration, input handling and further more.\nFor further details to different parameters like --extention_variant or --repeated_undersampling see the paper, **virus host prediction with deep learning**. \n\nexamples for vidhop training:\n\nset input output and name of the model\n```\nvidhop training -i /home/user/trainingdata/ -o /home/user/model/ --name test\n```\n\nuse the LSTM archtecture and the extention variant random repeat\n```\nvidhop training -i /home/user/trainingdata/ --architecture 0 --extention_variant 2\n```\n\nuse repeated undersampling for training, note that for this the dataset must have been created with repeated undersampling enabled\n```\nvidhop training -i /home/user/trainingdata/ -r\n```\n\ntrain the model for 40 epochs, stop training if for 2 epochs the accuracy did not increase\n```\nvidhop train_new_model -i /home/user/trainingdata/ --epochs 40 --early_stopping\n```\n\ncommand | what it does\n  ------------- | -------------\n-i, --inpath     |Path to the dir with training files, generated with make_dataset  [required].\n-o, --outpath   |Path where to save the output.\n-n, --name   |Suffix added to output file names.\n-e, --epochs    |Maximum number of epochs used for training the model.\n-a, --architecture   |Select architecture (0:LSTM, 1:CNN+LSTM).\n-v, --extention_variant   |Select extension variant (0:Normal repeat, 1:Normal repeat with gaps, 2:Random repeat, 3:Random repeat with gaps, 4:Append gaps, 5:Smallest, 6:Online).\n-s, --early_stopping   |Stop training when model accuracy did not improve over time, patience 5% of max epochs.\n-r, --repeated_undersampling       |Use repeated undersampling while training, to be usable the training files must be generated with make_datasets and activated reapeted undersampling parameter.\n--help          |Show this message and exit.\n\n### toy-example\n\nDownload the test files X.txt (containing all sequences) und Y.txt (containing all corresponding hosts). \n```\nwget https://github.com/flomock/vidhop/blob/master/X.txt\nwget https://github.com/flomock/vidhop/blob/master/Y.txt\n```\n\nNow we prepare the dataset. As an example we define the size of the validation-set to 10%  and the test-set to 20%\n of the full dataset. Note that the all data sets will be balanced according to their host classes. To use all samples, \n even from an unbalanced dataset, without biasing towards the most common host class, use the --repeated_undersampling \n parameter. This effects the samples used while training. The validation and test sets will be unchanged.\n\n```\nvidhop make_dataset -x X.txt -y Y.txt -r -o ./make_dataset_out -v 0.1 -t 0.2\n```\nIf a host in your dataset is bellow the recommended minimal count of 100 samples, vidhop make_dataset will return a \nwarning. The expected console output:\n\n```\nwarning number samples for host Artibeus lituratus low, only 90 samples\nwarning number samples for host Lasiurus borealis low, only 96 samples\nwarning number samples for host Cerdocyon thous low, only 82 samples\n```\n\nNow we train a model using the standard parameter. As input we provide the output directory of vidhop make_dataset. \nFurthermore we name our model \"test_standard\". \nTo limit training time we use -e to limit the number of epochs to two.  \n\n```\nvidhop training -i ./make_dataset_out -n test_standard -e 2 -o ./trained_models\n```\n\nThe output printed to the console provides information about the input provided, the current architecture used, the \ncurrent status of the training. When the actual training is completed two models are saved. One model which represents \nthe model with the lowest loss during training and one model with the highest accuracy during training, both calculated \non the validation set. \nAfter training and saving these models each model is tested on the test dataset. The results are printed in the console.\n\nNow we are able to predict the host of new sequences. For this we use [vidhop predict](#examples-for-vidhop-predict:).\nYou can provide either a fasta file for prediction or a DNA sequence directly. Here we use an DNA sequence. Furthermore \nwe define a the virus to use the path to one of our trained models, either the one with the lowest loss or the one with \nthe highest accuracy while training. If you are not sure which of the both models to use, we experienced the best \nresults working with the model with the highest accuracy.\n\n```\nvidhop predict -v ./trained_models/model_best_acc_test_standard.model -o ./predictions/first_test.txt -i AAATGCTCTGAATTCGACATGAAAAAAACAAGCAACACCACTGATAAGATGAACTTTCTACGCAAGAAATGCTCTGAATTCGACATGAAAAAAACAAGCAACACCACTGATAAGATGAACTTTCTACGCAAG\n```\n\nThis results in a prediction similar to:\n```\n>user command line input\nall hosts\nLasiurus borealis: 0.061880290508270264\nProcyon lotor: 0.06131688505411148\nDesmodus rotundus: 0.06110725179314613\nMephitis mephitis: 0.060785286128520966\nVulpes vulpes: 0.060404110699892044\nCapra hircus: 0.06007476523518562\nVulpes lagopus: 0.059015918523073196\nArtibeus lituratus: 0.0590040422976017\nTadarida brasiliensis: 0.05887320265173912\nNyctereutes procyonoides: 0.0584951676428318\nEptesicus fuscus: 0.05815460532903671\nFelis catus: 0.05793345347046852\nEquus caballus: 0.0575258694589138\nHomo sapiens: 0.05704779922962189\nBos taurus: 0.05660048499703407\nCanis lupus: 0.056392643600702286\nCerdocyon thous: 0.05538821220397949\n```\n(note that the input sequence is more or less random, so don't expect a very meaningful prediction)\n\n\nThanks for using VIDHOP.\n\n<br><br>\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n"
}