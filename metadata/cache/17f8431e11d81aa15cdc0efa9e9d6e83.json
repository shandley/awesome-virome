{
  "cache_date": "2025-08-01T11:06:49.011535",
  "data": "![](https://img.shields.io/badge/nextflow-22.04.5-brightgreen)\n![](https://img.shields.io/badge/uses-docker-blue.svg)\n![](https://img.shields.io/badge/uses-singularity-red.svg)\n\n<img align=\"right\" width=\"140\" height=\"140\" src=\"figures/virify_logo.png\">\n\n1. [ The VIRify pipeline ](#virify)\n2. [ Nextflow execution ](#nf)\n3. [ Pipeline overview ](#overview)\n4. [ Detour: Metatranscriptomics ](#metatranscriptome)\n5. [ Resources ](#resources)\n6. [ Citations ](#cite)\n\n<a name=\"virify\"></a>\n\n# VIRify\n![Sankey plot](figures/2023-sankey-neto.png)\n\n## General\nVIRify is a pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. The pipeline is part of the repertoire of analysis services offered by [MGnify](https://www.ebi.ac.uk/metagenomics/). VIRify's taxonomic classification relies on the detection of taxon-specific profile hidden Markov models (HMMs), built upon a set of 22,013 orthologous protein domains and [referred to as ViPhOGs](https://doi.org/10.3390/v13061164). \n\nThe pipeline is implemented in [Nextflow](#nf) and additionally only Docker or Singularity are needed to run VIRify. Details about installation and usage are given below.\n\n\n<a name=\"nf\"></a>\n\n# Nextflow\n\nA [Nextflow](https://www.nextflow.io/) implementation of the VIRify pipeline.\n\n## What do I need?\n\nThis implementation of the pipeline runs with the workflow manager [Nextflow](https://www.nextflow.io/) and needs as second dependency either [Docker](https://docs.docker.com/v17.09/engine/installation/linux/docker-ce/ubuntu/#install-docker-ce) or [Singularity](https://sylabs.io/guides/3.0/user-guide/quick_start.html). Conda will be implemented soonish, hopefully (currently blocked bc/ we use [PPR-Meta](https://github.com/zhenchengfang/PPR-Meta/issues/8)). However, we highly recommend in any way the usage of the stable containers. All other programs and databases are automatically downloaded by Nextflow. \n\n**Attention**, the workflow will download the containers and databases with a size of roughly 19 GB (49 GB with `--hmmextend` and `--blastextend`) the first time it is executed! \n\n### Install Nextflow\n```bash\ncurl -s https://get.nextflow.io | bash\n```\n* for troubleshooting, see [more instructions about Nextflow](https://www.nextflow.io/). \n\n### Install Docker\nIf you dont have experience with bioinformatic tools and their installation just copy the commands into your terminal to set everything up (local machine with full permissions!):\n```bash\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\nsudo usermod -a -G docker $USER\n```\n* restart your computer\n* for troubleshooting, see [more instructions about Docker](https://docs.docker.com/v17.09/engine/installation/linux/docker-ce/ubuntu/#install-docker-ce)\n\n### Install Singularity\n\nWhile singularity can be installed via Conda, we recommend setting up a _true_ Singularity installation. For HPCs, ask the system administrator you trust. [Here](https://github.com/hpcng/singularity/blob/master/INSTALL.md) is also a good manual to get you started. _Please note_: you only need Docker or Singularity. However, due to security concerns it might not be possible to use Docker on your shared machine or HPC.\n\n## Basic Nextflow execution\n\n### Install\n\nWhile it is possible to clone this repository and directly execute the `virify.nf`, but we _recommend_ letting Nextflow handle the installation.\n\nGet the pipeline code via:\n```bash\nnextflow pull EBI-Metagenomics/emg-viral-pipeline\n```\n\nTest installation and get help:\n```bash\nnextflow run EBI-Metagenomics/emg-viral-pipeline --help\n```\n\n### Run specific pipeline version\n\nWe __highly recommend__ to always run from a [release](https://github.com/EBI-Metagenomics/emg-viral-pipeline/releases):\n\n```bash\nnextflow run EBI-Metagenomics/emg-viral-pipeline -r v3.0.0 --help\n```\n\nCheck the [release page](https://github.com/EBI-Metagenomics/emg-viral-pipeline/releases) to figure out the newest version of the pipelne. Or run:\n\n```bash\nnextflow info EBI-Metagenomics/emg-viral-pipeline\n```\n\n### Input\n\nThe pipeline accepts the assembly either using the `--fasta` parameter for a single one. Or the `--samplesheet` parameter to indicate a .csv that contains as many assemblies. The former allows for greater parallelization as one head nextflow job can run many assemblies at once.\nSamplesheet\n\n#### Samplesheet\n\nThe samplesheet must be a .csv file that contains the following columns:\n\n- id - Sample identifier (mandatory)\n- assembly - Assembly file in FASTA format (optional)\n- fastq_1 - FastQ file for reads 1 in '.fq.gz' or '.fastq.gz' format (optional)\n- fastq_2 - FastQ file for reads 2 in '.fq.gz' or '.fastq.gz' format\n- protein - Proteins file in FASTA format (optional)\n \nThe fastq_1 and fastq_2 files are optional and can be provided if the user wants the reads to be assembled. The proteins file is also optional and can be provided to avoid calling the protein caller again.\n\n[Example](assets/example_input.csv)\n```\nid,assembly,fastq_1,fastq_2,proteins\nERZ123,ERZ123.fasta,,,\n```\n\n### Example execution\n\nRun annotation for a small assembly file (10 contigs, 0.78 Mbp) on your local Linux machine using Docker containers (per default `--cores 4`; takes approximately 10 min on a 8 core i7 laptop + time for database download; ~19 GB):\n\n```bash\nnextflow run EBI-Metagenomics/emg-viral-pipeline -r v3.0.0 \\\\\n    --fasta \"/home/$USER/.nextflow/assets/EBI-Metagenomics/emg-viral-pipeline/nextflow/test/assembly.fasta\" \\\\\n    --cores 4 -profile local,docker\n```\n\n__Please note__ that in particular the following parameters are important to handle where Nextflow writes files. \n\n* `--workdir` or `-w` (here your work directories with intermediate data will be saved)\n* `--databases` (here your databases will be saved and the workflow checks if they are already available under this path)\n* `--singularity_cachedir` (here Singularity containers will be cached, not needed for Docker, default path: `./singularity`)\n\n**Please clean up your work directory from time to time to save disk space!**\n\n## Profiles\n\nNextflow uses a merged profile handling system so you have to define an executor (e.g., `local`, `lsf`, `slurm`) and an engine (`docker`, `singularity`) to run the pipeline according to your needs and infrastructure. \n\nPer default, the workflow runs locally (e.g., on your laptop) with Docker. When you execute the workflow on a HPC you can for example switch to a specific job scheduler and Singularity instead of Docker:\n\n* SLURM (``-profile slurm,singularity``)\n* LSF (``-profile lsf,singularity``)\n\nDon't forget, especially on an HPC, to define further important parameters such as `-w`, `--databases`, and `--singularity_cachedir` as mentioned above.\n\nThe engine `conda` is not working at the moment until there is a conda recipe for PPR-Meta or we switch the tool. Sorry. Use Docker. Or Singularity. Please. Or install PPR-Meta by yourself and then use the `conda` profile (not recommended).  \n\n## Monitoring\n\n<img align=\"right\" width=\"400px\" src=\"figures/tower.png\" alt=\"Monitoring with Nextflow Tower\" /> \n\nTo monitor your Nextflow computations, VIRify can be connected to [Nextflow Tower](https://tower.nf). You need a user access token to connect your Tower account with the pipeline. Simply [generate a login](https://tower.nf/login) using your email and then click the link sent to this address.\n\nOnce logged in, click on your avatar in the top right corner and select \"Your tokens.\" Generate a token or copy the default one and set the following environment variable:\n\n```bash\nexport TOWER_ACCESS_TOKEN=<YOUR_COPIED_TOKEN>\n```\n\nYou can save this variable in your `.bashrc` or `.profile` to not need to enter it again. Refresh your terminal.\n\nNow run:\n\n```bash\nnextflow run EBI-Metagenomics/emg-viral-pipeline -r v0.4.0 \\\\\n    --fasta \"/home/$USER/.nextflow/assets/EBI-Metagenomics/emg-viral-pipeline/nextflow/test/assembly.fasta\" \\\\\n    --cores 4 -profile local,docker \\\\\n    -with-tower\n```\n\nAlternatively, you can also pull the code from this repository and activate the Tower connection within the `nextflow.config` file located in the root GitHub directory:\n\n```java\ntower {\n    accessToken = ''\n    enabled = true\n} \n```\n\nYou can also directly enter your access token here instead of generating the above-mentioned environment variable.\n\n## Results\nThe outputs generated from viral prediction tools, ViPhOG annotation, taxonomy assign, and CheckV quality are integrated and summarized in a validated gff file.\nBy default pipeline produces `08-final` folder with the following structure:\n<details>\n<summary>Structure example per-assembly</summary>\n\n    08-final\n        \u251c\u2500\u2500 annotation\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 hmmer\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_prodigal_annotation.tsv\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_prodigal_annotation.tsv\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_prodigal_annotation.tsv\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 plot_contig_map\n        \u2502\u00a0\u00a0     \u251c\u2500\u2500 high_confidence_viral_contigs_mapping_results\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_prot_ann_table_filtered.tsv\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 plot_pdfs.tar.gz\n        \u2502\u00a0\u00a0     \u251c\u2500\u2500 low_confidence_viral_contigs_mapping_results\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_prot_ann_table_filtered.tsv\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 plot_pdfs.tar.gz\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 prophages_mapping_results\n        \u2502\u00a0\u00a0         \u251c\u2500\u2500 plot_pdfs.tar.gz\n        \u2502\u00a0\u00a0         \u2514\u2500\u2500 prophages_prot_ann_table_filtered.tsv\n        \u251c\u2500\u2500 contigs\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_original.fasta\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_original.fasta\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_original.fasta\n        \u251c\u2500\u2500 chromomap [optional step]\n        \u251c\u2500\u2500 gff\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 ACCESSION_virify.gff\n        \u251c\u2500\u2500 krona\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 ACCESSION.krona.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs.krona.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs.krona.html\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages.krona.html\n        \u2514\u2500\u2500 sankey\n            \u251c\u2500\u2500 ACCESSION.sankey.html\n            \u251c\u2500\u2500 high_confidence_viral_contigs.sankey.html\n            \u251c\u2500\u2500 low_confidence_viral_contigs.sankey.html\n            \u2514\u2500\u2500 prophages.sankey.html\n</details>\n\nIn order to have expanded output with more files use `--publish_all` option in pipeline execution.\n\n<details>\n<summary>Expanded structure example per-assembly</summary>\n\n    \u251c\u2500\u2500 01-predictions\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ACCESSION_virus_predictions.stats\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 pprmeta\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ACCESSION_pprmeta.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 virfinder\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ACCESSION.txt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 virsorter2\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 final-viral-boundary.tsv\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 final-viral-combined.fa\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 final-viral-score.tsv\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 virsorter_metadata.tsv\n    \u251c\u2500\u2500 02-prodigal\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_prodigal.faa\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 low_confidence_viral_contigs_prodigal.faa\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_prodigal.faa  \n    \u251c\u2500\u2500 03-hmmer\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_modified.tsv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_modified.tsv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 prophages_modified.tsv \n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ratio_evalue_tables\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_modified_informative.tsv\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_modified_informative.tsv\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_modified_informative.tsv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vpHMM_database_v3\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 high_confidence_viral_contigs_vpHMM_database_v3_hmmsearch.tbl\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 low_confidence_viral_contigs_vpHMM_database_v3_hmmsearch.tbl\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 prophages_vpHMM_database_v3_hmmsearch.tbl\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 [other chosen optional HMM DBs]\n    \u251c\u2500\u2500 04-blast [optional step]\n    \u251c\u2500\u2500 05-plots\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 krona\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ACCESSION.krona.tsv\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs.krona.tsv\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs.krona.tsv\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages.krona.tsv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sankey\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 all.sankey.filtered-25.json\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 all.sankey.tsv\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 high_confidence_viral_contigs.sankey.filtered-25.json\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 high_confidence_viral_contigs.sankey.tsv\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 low_confidence_viral_contigs.sankey.filtered-25.json\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 low_confidence_viral_contigs.sankey.tsv\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 prophages.sankey.filtered-25.json\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 prophages.sankey.tsv\n    \u251c\u2500\u2500 06-taxonomy\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_prodigal_annotation_taxonomy.tsv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_prodigal_annotation_taxonomy.tsv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_prodigal_annotation_taxonomy.tsv\n    \u251c\u2500\u2500 07-checkv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_quality_summary.tsv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_quality_summary.tsv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_quality_summary.tsv\n    \u2514\u2500\u2500 08-final\n        \u251c\u2500\u2500 annotation\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 hmmer\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_prodigal_annotation.tsv\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_prodigal_annotation.tsv\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_prodigal_annotation.tsv\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 plot_contig_map\n        \u2502\u00a0\u00a0     \u251c\u2500\u2500 high_confidence_viral_contigs_mapping_results\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_prot_ann_table_filtered.tsv\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 plot_pdfs.tar.gz\n        \u2502\u00a0\u00a0     \u251c\u2500\u2500 low_confidence_viral_contigs_mapping_results\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_prot_ann_table_filtered.tsv\n        \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 plot_pdfs.tar.gz\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 prophages_mapping_results\n        \u2502\u00a0\u00a0         \u251c\u2500\u2500 plot_pdfs.tar.gz\n        \u2502\u00a0\u00a0         \u2514\u2500\u2500 prophages_prot_ann_table_filtered.tsv\n        \u251c\u2500\u2500 contigs\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs_original.fasta\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs_original.fasta\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages_original.fasta\n        \u251c\u2500\u2500 chromomap [optional step]\n        \u251c\u2500\u2500 gff\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 ACCESSION_virify.gff\n        \u251c\u2500\u2500 krona\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 ACCESSION.krona.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 high_confidence_viral_contigs.krona.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 low_confidence_viral_contigs.krona.html\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 prophages.krona.html\n        \u2514\u2500\u2500 sankey\n            \u251c\u2500\u2500 ACCESSION.sankey.html\n            \u251c\u2500\u2500 high_confidence_viral_contigs.sankey.html\n            \u251c\u2500\u2500 low_confidence_viral_contigs.sankey.html\n            \u2514\u2500\u2500 prophages.sankey.html\n</details>\n\n\n### GFF output files\n\nYou can find such output in the `08-final/gff/` folder.\n\nThe labels used in the Type column of the gff file correspond to the following nomenclature according to the [Sequence Ontology resource](http://www.sequenceontology.org/browser/current_svn/term/SO:0000001):\n\n| Type in gff file  | Sequence ontology ID |\n| ------------- | ------------- |\n| viral_sequence  | [SO:0001041](http://www.sequenceontology.org/browser/current_svn/term/SO:0001041) |\n| prophage  | [SO:0001006](http://www.sequenceontology.org/browser/current_svn/term/SO:0001006) |\n| CDS | [SO:0000316](http://www.sequenceontology.org/browser/current_svn/term/SO:0000316) |\n\nNote that CDS are reported only when a ViPhOG match has been found.\n\n<a name=\"overview\"></a>\n\n# Pipeline overview\n![VIRify Overview](figures/virify_fig1_workflow.png)\nFor further details please check: [doi.org/10.1101/2022.08.22.504484](https://doi.org/10.1101/2022.08.22.504484)\n\n\n<a name=\"metatranscriptome\"></a>\n\n# A note about metatranscriptomes\n\nAlthough VIRify has been benchmarked and validated with metagenomic data in mind, it is also possible to use this tool to detect RNA viruses in metatranscriptome assemblies (e.g. SARS-CoV-2). However, some additional considerations for this purpose are outlined below:\n\n**1. Quality control:** As for metagenomic data, a thorough quality control of the FASTQ sequence reads to remove low-quality bases, adapters and host contamination (if appropriate) is required prior to assembly. This is especially important for metatranscriptomes as small errors can further decrease the quality and contiguity of the assembly obtained. We have used [TrimGalore](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/) for this purpose.\n\n**2. Assembly:** There are many assemblers available that are appropriate for either metagenomic or single-species transcriptomic data. However, to our knowledge, there is no assembler currently available specifically for metatranscriptomic data. From our preliminary investigations, we have found that transcriptome-specific assemblers (e.g. [rnaSPAdes](http://cab.spbu.ru/software/spades/)) generate more contiguous and complete metatranscriptome assemblies compared to metagenomic alternatives (e.g. [MEGAHIT](https://github.com/voutcn/megahit/releases) and [metaSPAdes](http://cab.spbu.ru/software/spades/)).\n\n**3. Post-processing:** Metatranscriptomes generate highly fragmented assemblies. Therefore, filtering contigs based on a set minimum length has a substantial impact in the number of contigs processed in VIRify. It has also been observed that the number of false-positive detections of [VirFinder](https://github.com/jessieren/VirFinder/releases) (one of the tools included in VIRify) is lower among larger contigs. The choice of a length threshold will depend on the complexity of the sample and the sequencing technology used, but in our experience any contigs <2 kb should be analysed with caution.\n\n**4. Classification:** The classification module of VIRify depends on the presence of a minimum number and proportion of phylogenetically-informative genes within each contig in order to confidently assign a taxonomic lineage. Therefore, short contigs typically obtained from metatranscriptome assemblies remain generally unclassified. For targeted classification of RNA viruses (for instance, to search for Coronavirus-related sequences), alternative DNA- or protein-based classification methods can be used. Two of the possible options are: (i) using [MashMap](https://github.com/marbl/MashMap/releases) to screen the VIRify contigs against a database of RNA viruses (e.g. Coronaviridae) or (ii) using [hmmsearch](http://hmmer.org/download.html) to screen the proteins obtained in the VIRify contigs against marker genes of the taxon of interest.\n\n<a name=\"resources\"></a>\n\n# Resources\n\nAdditional material (assemblies used for benchmarking in the paper, ...) as well as the ViPhOG HMMs with model-specific bit score thresholds used in VIRify are available at [osf.io/fbrxy](https://osf.io/fbrxy/).\n\nHere, we also list databases used and automatically downloaded by the pipeline **(in v2.0.0)** when it is first run. We deposited database files on a separate FTP to ensure their accessibility. The files can be also downloaded manually and then used as an input for the pipeline to prevent the auto-download (see `--help` in the Nextflow pipeline).\n\n### Virus-specific protein profile HMMs\n\n* **ViPhOGs** (mandatory, used for taxonomy assignment)\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vpHMM_database_v3.tar.gz`\n    * Additional metadata file for filtering the ViPhOGs (according to taxonomy updates by the [ICTV](https://ictv.global/taxonomy))\n        * `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/additional_data_vpHMMs_v4.tsv`\n    * [Publication](https://www.mdpi.com/1999-4915/13/6/1164)\n* **pVOGs** (optional)\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/pvogs.tar.gz`\n    * [Publication](https://doi.org/10.1093/nar/gkw975)\n* **RVDB** (optional)\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/rvdb.tar.gz`\n    * [Publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7492780/)\n* **VOGDB** (optional)\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vogdb.tar.gz`\n    * [Publication](https://vogdb.org/)\n* **VPF** (optional)\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vpf.tar.gz`\n    * [Publication](https://doi.org/10.1093/nar/gky1127)\n\n### Initial virus prediction on contig level\n\n* **VirSorter** HMMs\n    * `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/virsorter-data-v2.tar.gz`\n    * [Publication](https://peerj.com/articles/985/)\n* **Virfinder** model\n    * `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/virfinder/VF.modEPV_k8.rda`\n    * [Publication](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-017-0283-5)\n\n### Virus prediction QC\n\n* **CheckV**\n    * `wget https://portal.nersc.gov/CheckV/checkv-db-v1.0.tar.gz`\n    * [Publication](https://www.nature.com/articles/s41587-020-00774-7)\n\n### Taxonomy annotation\n\n* **NCBI taxonomy**\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/2022-11-01_ete3_ncbi_tax.sqlite.gz`\n\n### Additional blast-based assignment (optional, super slow)\n\n* **IMG/VR**\n    * `wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/IMG_VR_2018-07-01_4.tar.gz`\n    * [Publication](https://doi.org/10.1093/nar/gkw1030)\n\n\n<a name=\"cite\"></a>\n\n# Cite\n\nIf you use the pipeline or ViPhOG HMMs in your work, please cite accordingly:\n\n**ViPhOGs:**\n\n[Moreno-Gallego, Jaime Leonardo, and Alejandro Reyes. \"Informative regions in viral genomes.\" _Viruses_ 13.6 (2021): 1164.](https://www.mdpi.com/1999-4915/13/6/1164)\n\n**VIRify:** \n\n[Rangel-Pineros, Guillermo, et al. \"VIRify: an integrated detection, annotation and taxonomic classification pipeline using virus-specific protein profile hidden Markov models.\" _bioRxiv_ (2022)](https://doi.org/10.1101/2022.08.22.504484)\n"
}