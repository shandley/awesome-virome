{
  "cache_date": "2025-03-17T02:18:19.238417",
  "data": "# Seq2Vec\nVersion 1.1 <br>\nAuthors: Yan Miao, Fu Liu, Yun Liu <br>\nMaintainer: Yan Miao miaoyan17@mails.jlu.edu.cn \n\n# Description\n  This package provides a novel codon embedding method for identification of viral contigs from metagenomic data in a fasta file. The method has the ability to identify viral contigs with short length (<500bp) from metagenomic data.\n\n  Seq2Vec generally tries to map a nucleotide sequence using a codon dictionary to a vector. Before embedded into a vector, a nucleotide sequence is preprocessed into a string of codons with a stride of some bases. Take a look at the nucleotide sequence \u201cATAGCCTGAAAGC\u201d for an example. It is firstly converted into a format of gene sentence \u201cATA, TAG, AGC, GCC, CCT, CTG, TGA, GAA, AAA, AAG, AGC\u201d with a stride of one base (see `train_500bp.csv`), where each codon can be regarded as a word in a sentence. The list of all 64 unique codons composes the whole complete dictionary, which is then used to codon embedding. Thus, a dictionary may look like \u2013 [\u2018AAA\u2019, \u2018AAT\u2019, \u2018AAG\u2019, \u2018AAC\u2019, \u2018ATA\u2019, \u2018ATT\u2019, \u2026\u2026]. In the training step, the nucleotide sentence \u201cATAGCCTGAAAGCTTGGATTG\u201d in the training dataset with one-hot encoded codons are firstly separated into a training set for a skip-gram model with context window of 1. Next, the input codons in the formed training set are continuously inputted to the skip-gram model, and then multiplied by the weights between input layer and hidden layer into the hidden activations, which latter get multiplied by the hidden-output weights to calculate the final outputs. Then the costs calculated by the negative log likelihood between final outputs and targets produced in the training set are back-propagated to learn the weights. Finally, the weights between the hidden layer and the output layer are taken as the codon vector representations of the codons, namely the embedding matrix.\n  The prediction model is an attention based LSTM neural network that learns the high-level features of each contig to distinguish virus from host. The model is trained using equal number of known viral and host sequences from NCBI RefSeq database. Then the sequence is predicted by the LSTM model trained with previously known sequences.\n\n# Dependencies\nTo utilize Seq2Vec, Python packages \"tflearn\", \"sklearn\", \"numpy\" and \"matplotlib\" are needed to be previously installed. Some other packages are also needed to make sure the code running correctly such as \"os\", \"ast\", etc.\n\nIn convenience, you can download Anaconda from https://repo.anaconda.com/archive/, where contains most of needed packages. If there are still some special packages that are missed when running, you can use \"pip install\" to install the specific packages. \n\nTo install tensorflow, start \"cmd.exe\" and enter <br>\n```\npip install tensorflow\n```\nTo insatll Keras, start \"cmd.exe\" and enter <br>\n```\npip install Keras\n```\nOur codes are all edited by Python 3.6.5 with TensorFlow 1.3.0.\n\n# Training the embedding matrix\nSeq2Vec has supplied a trained embedding matrix in `embedding matrix.csv`. The training dataset is chosen as the whole RefSeq genomes. The NCBI accession number can be found in `NCBI accession numbers of the whole Refseq genomes.xlsx`. If you would like to train the embedding matrix by youself, just run `embedding.py`.\n\n# Usage\nIt is simple to use Seq2Vec for users' database. <br>\nBefore training and testing, the query contigs should be preprocessed to an available format using `preprocessing.py`.\nThere are two ways for users to train the model by \"train.py\".\n* Using our original training database (containing 4500 viral sequences and 4500 host sequences of length 500bp) `\"train_500bp.csv\"`. If you would like to test query contigs with length of 300bp, you can use our training dataset `\"train_300bp.csv\"`. <br>\nUsers can retrain the model first, and then test the query contigs. When training, you can make some changes to the hyperparameters to get a better performance.\n* Using users' own database in a \".csv\" format. <br>\n\t* Firstly, chose a set of hyperparameters to train your dataset.\n\t* Secondly, train and refine your model using your dataset according to the performance on a related validation dataset.\n\t* Finally, utilize the fully trained model to identify query contigs. \nNote: Before training, set the path to where the database is located. If users will not train the embedding manually nor utilize our provided embedding matrix, you can use `Seq2Vec_train&test.py`, where an embedding layer has been added by \"tflearn.embedding()\".\n\nTo make a prediction by \"test.py\", users' own query contigs should be edited into a \".csv\" file, where every line contains a single query contig, the format of which should be set as discribed above. After training and testing, Seq2Vec will give a set of scores to each query contig, higher of which represents its classification result.\n\n# Copyright and License Information\nCopyright (C) 2021 Jilin University\n\nAuthors: Yan Miao, Fu Liu, Tao Hou, Yun Liu\n\nThis program is freely available as Python at https://github.com/crazyinter/Seq2Vec.\n\nCommercial users should contact Mr. Miao at miaoyan17@mails.jlu.edu.cn, copyright at Jilin University.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n"
}