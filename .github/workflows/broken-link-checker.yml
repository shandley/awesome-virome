name: Broken Link Checker

on:
  # Run weekly to check for broken links
  schedule:
    - cron: "0 0 * * 1"  # Every Monday at midnight UTC
  
  # Run after content updates
  workflow_run:
    workflows:
      - "Simplified Data Update Workflow"
    types:
      - completed
    branches:
      - main
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      check_all:
        description: 'Check all links (slower but more thorough)'
        type: boolean
        default: false

permissions:
  contents: read
  issues: write

jobs:
  check-links:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install link checker dependencies
        run: |
          pip install requests beautifulsoup4 markdown

      - name: Create link checker script
        run: |
          mkdir -p link_check_results
          
          # Create Python link checker script
          cat > check_links.py << 'EOF'
          #!/usr/bin/env python3
          
          import os
          import re
          import json
          import argparse
          import requests
          import markdown
          from bs4 import BeautifulSoup
          from concurrent.futures import ThreadPoolExecutor
          from urllib.parse import urljoin, urlparse
          
          def is_url(text):
              """Check if text is a URL."""
              return text.startswith(('http://', 'https://', 'ftp://'))
          
          def extract_links_from_markdown(markdown_content):
              """Extract links from markdown content."""
              # Convert markdown to HTML
              html = markdown.markdown(markdown_content)
              
              # Parse HTML
              soup = BeautifulSoup(html, 'html.parser')
              
              # Extract links from <a> tags
              links = []
              for a in soup.find_all('a', href=True):
                  href = a['href']
                  if is_url(href):
                      links.append(href)
              
              # Also find markdown links that might not have been converted properly
              # Pattern for markdown links: [text](url)
              md_links = re.findall(r'\[.*?\]\((http[s]?://[^)]+)\)', markdown_content)
              links.extend([link for link in md_links if link not in links])
              
              return links
          
          def extract_links_from_html(html_content):
              """Extract links from HTML content."""
              soup = BeautifulSoup(html_content, 'html.parser')
              links = []
              
              for a in soup.find_all('a', href=True):
                  href = a['href']
                  if is_url(href):
                      links.append(href)
              
              return links
          
          def extract_links_from_json(json_content, current_path=""):
              """Extract links from JSON content."""
              links = []
              
              if isinstance(json_content, dict):
                  for key, value in json_content.items():
                      new_path = f"{current_path}.{key}" if current_path else key
                      
                      if isinstance(value, str) and is_url(value):
                          links.append((value, new_path))
                      
                      # Recursively check nested structures
                      links.extend(extract_links_from_json(value, new_path))
                          
              elif isinstance(json_content, list):
                  for i, item in enumerate(json_content):
                      new_path = f"{current_path}[{i}]"
                      links.extend(extract_links_from_json(item, new_path))
                      
              return links
          
          def check_url(url_info):
              """Check if a URL is accessible."""
              if isinstance(url_info, tuple):
                  url, path = url_info
              else:
                  url = url_info
                  path = ""
                  
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Awesome-Virome-Link-Checker'
                  }
                  
                  # First try a HEAD request to be faster and less intrusive
                  response = requests.head(
                      url, 
                      timeout=10,
                      allow_redirects=True,
                      headers=headers
                  )
                  
                  # If HEAD request fails, try GET
                  if response.status_code >= 400:
                      response = requests.get(
                          url, 
                          timeout=10, 
                          stream=True,  # Don't download the entire content
                          allow_redirects=True,
                          headers=headers
                      )
                      # Close the connection to avoid downloading the entire response
                      response.close()
                  
                  result = {
                      'url': url,
                      'status_code': response.status_code,
                      'status': 'ok' if 200 <= response.status_code < 400 else 'error',
                  }
                  
                  if path:
                      result['path'] = path
                      
                  return result
                  
              except requests.RequestException as e:
                  result = {
                      'url': url,
                      'status_code': 0,
                      'status': 'error',
                      'error': str(e)
                  }
                  
                  if path:
                      result['path'] = path
                      
                  return result
          
          def check_file_links(file_path, file_type):
              """Check all links in a file."""
              results = []
              
              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                      
                      if file_type == 'markdown':
                          links = extract_links_from_markdown(content)
                          
                          # Check links in parallel
                          with ThreadPoolExecutor(max_workers=5) as executor:
                              link_results = list(executor.map(check_url, links))
                              
                          for result in link_results:
                              result['file'] = file_path
                              results.append(result)
                              
                      elif file_type == 'html':
                          links = extract_links_from_html(content)
                          
                          with ThreadPoolExecutor(max_workers=5) as executor:
                              link_results = list(executor.map(check_url, links))
                              
                          for result in link_results:
                              result['file'] = file_path
                              results.append(result)
                              
                      elif file_type == 'json':
                          try:
                              json_content = json.loads(content)
                              links = extract_links_from_json(json_content)
                              
                              with ThreadPoolExecutor(max_workers=5) as executor:
                                  link_results = list(executor.map(check_url, links))
                                  
                              for result in link_results:
                                  result['file'] = file_path
                                  results.append(result)
                          except json.JSONDecodeError:
                              print(f"Error: {file_path} is not valid JSON")
              except Exception as e:
                  print(f"Error processing {file_path}: {e}")
                  
              return results
          
          def main():
              parser = argparse.ArgumentParser(description='Check for broken links in repository files')
              parser.add_argument('--check-all', action='store_true', help='Check all file types')
              parser.add_argument('--output-dir', default='link_check_results', help='Output directory for results')
              args = parser.parse_args()
              
              os.makedirs(args.output_dir, exist_ok=True)
              
              # Always check markdown files
              markdown_files = ['README.md', 'API.md', 'CONTRIBUTING.md']
              markdown_results = []
              
              for md_file in markdown_files:
                  if os.path.exists(md_file):
                      print(f"Checking links in {md_file}...")
                      results = check_file_links(md_file, 'markdown')
                      markdown_results.extend(results)
                      
                      # Save individual results
                      with open(f"{args.output_dir}/{os.path.splitext(os.path.basename(md_file))[0]}_results.json", 'w') as f:
                          json.dump(results, f, indent=2)
              
              # Save markdown results
              with open(f"{args.output_dir}/markdown_results.json", 'w') as f:
                  json.dump(markdown_results, f, indent=2)
              
              # Check HTML and JSON files if requested
              if args.check_all:
                  # Check HTML files
                  html_results = []
                  for root, _, files in os.walk('.'):
                      if '.git' in root or 'node_modules' in root:
                          continue
                          
                      for file in files:
                          if file.endswith('.html'):
                              file_path = os.path.join(root, file)
                              print(f"Checking links in {file_path}...")
                              results = check_file_links(file_path, 'html')
                              html_results.extend(results)
                  
                  # Save HTML results
                  with open(f"{args.output_dir}/html_results.json", 'w') as f:
                      json.dump(html_results, f, indent=2)
                  
                  # Check JSON files
                  json_results = []
                  for root, _, files in os.walk('.'):
                      if '.git' in root or 'node_modules' in root:
                          continue
                          
                      for file in files:
                          if file.endswith('.json'):
                              file_path = os.path.join(root, file)
                              print(f"Checking links in {file_path}...")
                              results = check_file_links(file_path, 'json')
                              json_results.extend(results)
                  
                  # Save JSON results
                  with open(f"{args.output_dir}/json_results.json", 'w') as f:
                      json.dump(json_results, f, indent=2)
              
              # Create a summary report
              summary = {
                  'markdown_files': {
                      'checked': len([f for f in markdown_files if os.path.exists(f)]),
                      'total_links': len(markdown_results),
                      'broken_links': len([r for r in markdown_results if r['status'] == 'error'])
                  }
              }
              
              if args.check_all:
                  summary['html_files'] = {
                      'total_links': len(html_results),
                      'broken_links': len([r for r in html_results if r['status'] == 'error'])
                  }
                  
                  summary['json_files'] = {
                      'total_links': len(json_results),
                      'broken_links': len([r for r in json_results if r['status'] == 'error'])
                  }
              
              # Save summary
              with open(f"{args.output_dir}/summary.json", 'w') as f:
                  json.dump(summary, f, indent=2)
              
              print(f"\nSummary:")
              print(f"- Markdown files: {summary['markdown_files']['broken_links']} broken links out of {summary['markdown_files']['total_links']}")
              
              if args.check_all:
                  print(f"- HTML files: {summary['html_files']['broken_links']} broken links out of {summary['html_files']['total_links']}")
                  print(f"- JSON files: {summary['json_files']['broken_links']} broken links out of {summary['json_files']['total_links']}")
          
          if __name__ == "__main__":
              main()
          EOF
          
          chmod +x check_links.py
      
      - name: Run link checker
        id: run_check
        run: |
          if [[ "${{ github.event.inputs.check_all }}" == "true" ]]; then
            python check_links.py --check-all
          else
            python check_links.py
          fi
      
      - name: Upload link check results
        uses: actions/upload-artifact@v4
        with:
          name: link-check-results
          path: link_check_results/
      
      - name: Generate link check report
        run: |
          # Create a markdown report
          echo "# Broken Link Check Report" > link_check_results/link_report.md
          echo "" >> link_check_results/link_report.md
          echo "Generated on: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> link_check_results/link_report.md
          echo "" >> link_check_results/link_report.md
          
          # Add summary data
          echo "## Summary" >> link_check_results/link_report.md
          echo "" >> link_check_results/link_report.md
          echo "| File Type | Links Checked | Broken Links |" >> link_check_results/link_report.md
          echo "| --------- | ------------- | ------------ |" >> link_check_results/link_report.md
          
          # Add markdown stats
          TOTAL_MARKDOWN=$(jq '.markdown_files.total_links' link_check_results/summary.json)
          BROKEN_MARKDOWN=$(jq '.markdown_files.broken_links' link_check_results/summary.json)
          echo "| Markdown files | $TOTAL_MARKDOWN | $BROKEN_MARKDOWN |" >> link_check_results/link_report.md
          
          # Add HTML and JSON stats if they exist
          if [[ "${{ github.event.inputs.check_all }}" == "true" ]]; then
            TOTAL_HTML=$(jq '.html_files.total_links' link_check_results/summary.json)
            BROKEN_HTML=$(jq '.html_files.broken_links' link_check_results/summary.json)
            echo "| HTML files | $TOTAL_HTML | $BROKEN_HTML |" >> link_check_results/link_report.md
            
            TOTAL_JSON=$(jq '.json_files.total_links' link_check_results/summary.json)
            BROKEN_JSON=$(jq '.json_files.broken_links' link_check_results/summary.json)
            echo "| JSON files | $TOTAL_JSON | $BROKEN_JSON |" >> link_check_results/link_report.md
          fi
          
          echo "" >> link_check_results/link_report.md
          
          # Add broken links details
          echo "## Broken Links" >> link_check_results/link_report.md
          echo "" >> link_check_results/link_report.md
          
          # Add broken links from markdown files
          echo "### Markdown Files" >> link_check_results/link_report.md
          echo "" >> link_check_results/link_report.md
          
          jq -r '.[] | select(.status == "error") | "- [\(.url)](\(.url)) - Status: \(.status_code // "Connection Error") - File: \(.file)"' link_check_results/markdown_results.json >> link_check_results/link_report.md
          
          # Add broken links from HTML and JSON if available
          if [[ "${{ github.event.inputs.check_all }}" == "true" ]]; then
            echo "" >> link_check_results/link_report.md
            echo "### HTML Files" >> link_check_results/link_report.md
            echo "" >> link_check_results/link_report.md
            
            jq -r '.[] | select(.status == "error") | "- [\(.url)](\(.url)) - Status: \(.status_code // "Connection Error") - File: \(.file)"' link_check_results/html_results.json >> link_check_results/link_report.md
            
            echo "" >> link_check_results/link_report.md
            echo "### JSON Files" >> link_check_results/link_report.md
            echo "" >> link_check_results/link_report.md
            
            jq -r '.[] | select(.status == "error") | "- [\(.url)](\(.url)) - Status: \(.status_code // "Connection Error") - File: \(.file) - Path: \(.path // "unknown")"' link_check_results/json_results.json >> link_check_results/link_report.md
          fi
          
          # Save to reports directory
          mkdir -p reports
          cp link_check_results/link_report.md reports/
      
      - name: Create or update issue for broken links
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            // Load summary data
            const summaryData = JSON.parse(fs.readFileSync('link_check_results/summary.json', 'utf8'));
            const markdownBroken = summaryData.markdown_files.broken_links;
            let totalBroken = markdownBroken;
            
            // Add HTML and JSON counts if they exist
            let htmlBroken = 0;
            let jsonBroken = 0;
            
            if (summaryData.html_files) {
              htmlBroken = summaryData.html_files.broken_links;
              totalBroken += htmlBroken;
            }
            
            if (summaryData.json_files) {
              jsonBroken = summaryData.json_files.broken_links;
              totalBroken += jsonBroken;
            }
            
            // Skip if no broken links found
            if (totalBroken === 0) {
              console.log('No broken links found. Skipping issue creation.');
              return;
            }
            
            // Load report content
            const reportContent = fs.readFileSync('link_check_results/link_report.md', 'utf8');
            
            // Find if there's an existing open issue
            const query = `repo:${context.repo.owner}/${context.repo.repo} is:issue is:open label:broken-links`;
            const issues = await github.rest.search.issuesAndPullRequests({
              q: query
            });
            
            const issueTitle = `🔗 ${totalBroken} Broken Links Detected - ${new Date().toISOString().split('T')[0]}`;
            
            if (issues.data.items.length > 0) {
              // Update existing issue
              const existingIssue = issues.data.items[0];
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                title: issueTitle,
                body: reportContent
              });
              console.log(`Updated existing issue #${existingIssue.number} with ${totalBroken} broken links`);
            } else {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: issueTitle,
                body: reportContent,
                labels: ['broken-links', 'documentation']
              });
              console.log(`Created new issue for ${totalBroken} broken links`);
            }
            
      - name: Upload Link Check Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: link-check-report
          path: link_check_results/link_report.md