name: Broken Link Checker

on:
  # Run weekly to check for broken links
  schedule:
    - cron: "0 0 * * 1"  # Every Monday at midnight UTC
  
  # Run after content updates
  workflow_run:
    workflows:
      - "Simplified Data Update Workflow"
    types:
      - completed
    branches:
      - main
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      check_all:
        description: 'Check all links (slower but more thorough)'
        type: boolean
        default: false

permissions:
  contents: read
  issues: write

jobs:
  check-links:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'
      
      - name: Install link checker
        run: npm install -g markdown-link-check
      
      - name: Check README links
        id: check_readme
        run: |
          # Create output directory
          mkdir -p link_check_results
          
          # Check README markdown links
          markdown-link-check README.md -q -o link_check_results/readme_results.json
          
          # Count broken links
          BROKEN_COUNT=$(jq '.[] | select(.status == "error") | .link' link_check_results/readme_results.json | wc -l)
          echo "README broken links: $BROKEN_COUNT"
          echo "readme_broken=$BROKEN_COUNT" >> $GITHUB_OUTPUT
      
      - name: Check API.md links
        id: check_api
        run: |
          # Check API markdown links
          if [ -f API.md ]; then
            markdown-link-check API.md -q -o link_check_results/api_results.json
            
            # Count broken links
            BROKEN_COUNT=$(jq '.[] | select(.status == "error") | .link' link_check_results/api_results.json | wc -l)
            echo "API.md broken links: $BROKEN_COUNT"
            echo "api_broken=$BROKEN_COUNT" >> $GITHUB_OUTPUT
          else
            echo "API.md not found, skipping"
            echo "api_broken=0" >> $GITHUB_OUTPUT
          fi
      
      - name: Check CONTRIBUTING.md links
        id: check_contributing
        run: |
          # Check CONTRIBUTING markdown links
          if [ -f CONTRIBUTING.md ]; then
            markdown-link-check CONTRIBUTING.md -q -o link_check_results/contributing_results.json
            
            # Count broken links
            BROKEN_COUNT=$(jq '.[] | select(.status == "error") | .link' link_check_results/contributing_results.json | wc -l)
            echo "CONTRIBUTING.md broken links: $BROKEN_COUNT"
            echo "contributing_broken=$BROKEN_COUNT" >> $GITHUB_OUTPUT
          else
            echo "CONTRIBUTING.md not found, skipping"
            echo "contributing_broken=0" >> $GITHUB_OUTPUT
          fi
          
      - name: Check HTML files
        id: check_html
        if: ${{ github.event.inputs.check_all == 'true' }}
        run: |
          # Install htmltest
          curl -sLO https://github.com/wjdp/htmltest/releases/download/v0.16.0/htmltest_0.16.0_linux_amd64.tar.gz
          tar -xzf htmltest_0.16.0_linux_amd64.tar.gz
          
          # Create htmltest config file
          cat > .htmltest.yml << EOF
          DirectoryPath: .
          CheckExternal: true
          IgnoreURLs:
            - "^https://github.com/(?!shandley/awesome-virome)"
          IgnoreInternalEmptyHash: true
          IgnoreDirectoryMissingTrailingSlash: true
          EOF
          
          # Run htmltest and save output
          HTML_FILES=$(find . -name "*.html" -not -path "./node_modules/*" -not -path "./.git/*")
          ./htmltest $HTML_FILES -o link_check_results/html_results.json || true
          
          # Count broken links
          if [ -f link_check_results/html_results.json ]; then
            BROKEN_COUNT=$(jq '.failedChecks | length' link_check_results/html_results.json)
            echo "HTML files broken links: $BROKEN_COUNT"
            echo "html_broken=$BROKEN_COUNT" >> $GITHUB_OUTPUT
          else
            echo "No HTML results found"
            echo "html_broken=0" >> $GITHUB_OUTPUT
          fi
      
      - name: Check JSON files for URLs
        id: check_json
        if: ${{ github.event.inputs.check_all == 'true' }}
        run: |
          # Set up Python
          python -m pip install --upgrade pip
          pip install requests

          # Create a Python script to check URLs in JSON files
          cat > check_json_urls.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import re
          import sys
          import requests
          from concurrent.futures import ThreadPoolExecutor
          
          def extract_urls_from_json(json_data, current_path=""):
              """Recursively extract URLs from JSON data."""
              urls = []
              
              if isinstance(json_data, dict):
                  for key, value in json_data.items():
                      new_path = f"{current_path}.{key}" if current_path else key
                      
                      # Check if this value might be a URL
                      if isinstance(value, str) and (
                          value.startswith("http://") or 
                          value.startswith("https://") or
                          value.startswith("ftp://")
                      ):
                          urls.append((value, new_path))
                      
                      # Recursively check nested structures
                      urls.extend(extract_urls_from_json(value, new_path))
                          
              elif isinstance(json_data, list):
                  for i, item in enumerate(json_data):
                      new_path = f"{current_path}[{i}]"
                      urls.extend(extract_urls_from_json(item, new_path))
                      
              return urls
          
          def check_url(url_info):
              """Check if a URL is valid."""
              url, path = url_info
              try:
                  # Use HEAD request first to minimize data transfer
                  response = requests.head(
                      url, 
                      timeout=10,
                      allow_redirects=True,
                      headers={'User-Agent': 'Mozilla/5.0 AwesomeVirome LinkChecker'}
                  )
                  
                  # If HEAD fails, try GET
                  if 400 <= response.status_code < 600:
                      response = requests.get(
                          url, 
                          timeout=10, 
                          allow_redirects=True,
                          headers={'User-Agent': 'Mozilla/5.0 AwesomeVirome LinkChecker'}
                      )
                      
                  return {
                      'url': url,
                      'path': path,
                      'status': response.status_code,
                      'valid': 200 <= response.status_code < 400
                  }
              except Exception as e:
                  return {
                      'url': url,
                      'path': path,
                      'status': 0,
                      'valid': False,
                      'error': str(e)
                  }
          
          def main():
              results = {'checked': 0, 'valid': 0, 'broken': 0, 'details': []}
              
              # Find all JSON files
              json_files = []
              for root, _, files in os.walk('.'):
                  if '.git' in root or 'node_modules' in root:
                      continue
                  for file in files:
                      if file.endswith('.json'):
                          json_files.append(os.path.join(root, file))
              
              all_urls = []
              
              # Process each JSON file
              for json_file in json_files:
                  try:
                      with open(json_file, 'r', encoding='utf-8') as f:
                          data = json.load(f)
                          urls = extract_urls_from_json(data)
                          for url, path in urls:
                              all_urls.append((url, f"{json_file}:{path}"))
                  except Exception as e:
                      print(f"Error processing {json_file}: {e}", file=sys.stderr)
              
              # Deduplicate URLs (keeping the first path)
              unique_urls = {}
              for url, path in all_urls:
                  if url not in unique_urls:
                      unique_urls[url] = path
              
              deduplicated_urls = [(url, path) for url, path in unique_urls.items()]
              print(f"Found {len(deduplicated_urls)} unique URLs in JSON files")
              
              # Check URLs in parallel
              with ThreadPoolExecutor(max_workers=5) as executor:
                  url_results = list(executor.map(check_url, deduplicated_urls))
              
              # Compile results
              results['checked'] = len(url_results)
              results['valid'] = sum(1 for r in url_results if r['valid'])
              results['broken'] = sum(1 for r in url_results if not r['valid'])
              results['details'] = sorted(url_results, key=lambda x: (x['valid'], x['url']))
              
              # Save results to file
              with open('link_check_results/json_results.json', 'w') as f:
                  json.dump(results, f, indent=2)
              
              print(f"Checked {results['checked']} URLs in JSON files")
              print(f"Valid: {results['valid']}")
              print(f"Broken: {results['broken']}")
              
              return results['broken']
          
          if __name__ == "__main__":
              broken_count = main()
              # Return count as exit code (but cap at 127 to avoid issues)
              sys.exit(min(broken_count, 127))
          EOF
          
          # Make the script executable
          chmod +x check_json_urls.py
          
          # Run the script
          python check_json_urls.py || true
          
          # Count broken links
          if [ -f link_check_results/json_results.json ]; then
            BROKEN_COUNT=$(jq '.broken' link_check_results/json_results.json)
            echo "JSON files broken links: $BROKEN_COUNT"
            echo "json_broken=$BROKEN_COUNT" >> $GITHUB_OUTPUT
          else
            echo "No JSON results found"
            echo "json_broken=0" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload link check results
        uses: actions/upload-artifact@v4
        with:
          name: link-check-results
          path: link_check_results/
      
      - name: Create or update issue for broken links
        uses: actions/github-script@v6
        if: ${{ steps.check_readme.outputs.readme_broken != '0' || steps.check_api.outputs.api_broken != '0' || steps.check_contributing.outputs.contributing_broken != '0' || steps.check_html.outputs.html_broken != '0' || steps.check_json.outputs.json_broken != '0' }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            // Count total broken links
            const readme_broken = parseInt('${{ steps.check_readme.outputs.readme_broken || 0 }}');
            const api_broken = parseInt('${{ steps.check_api.outputs.api_broken || 0 }}');
            const contributing_broken = parseInt('${{ steps.check_contributing.outputs.contributing_broken || 0 }}');
            const html_broken = parseInt('${{ steps.check_html.outputs.html_broken || 0 }}');
            const json_broken = parseInt('${{ steps.check_json.outputs.json_broken || 0 }}');
            
            const total_broken = readme_broken + api_broken + contributing_broken + html_broken + json_broken;
            
            // Create issue body
            let issueBody = `## Broken Links Detected - ${new Date().toISOString().split('T')[0]}
            
            The link checker found ${total_broken} broken link(s) in the repository:
            
            | Document | Broken Links |
            | -------- | ------------ |
            | README.md | ${readme_broken} |
            | API.md | ${api_broken} |
            | CONTRIBUTING.md | ${contributing_broken} |
            | HTML files | ${html_broken} |
            | JSON files | ${json_broken} |
            
            `;
            
            // Add details from results files
            try {
              if (readme_broken > 0 && fs.existsSync('link_check_results/readme_results.json')) {
                const readmeResults = JSON.parse(fs.readFileSync('link_check_results/readme_results.json', 'utf8'));
                issueBody += "\n### README.md Broken Links\n\n";
                readmeResults.filter(item => item.status === "error").forEach(item => {
                  issueBody += `- [${item.link}](${item.link}) - ${item.statusCode || 'Unknown error'}\n`;
                });
              }
              
              if (api_broken > 0 && fs.existsSync('link_check_results/api_results.json')) {
                const apiResults = JSON.parse(fs.readFileSync('link_check_results/api_results.json', 'utf8'));
                issueBody += "\n### API.md Broken Links\n\n";
                apiResults.filter(item => item.status === "error").forEach(item => {
                  issueBody += `- [${item.link}](${item.link}) - ${item.statusCode || 'Unknown error'}\n`;
                });
              }
              
              if (contributing_broken > 0 && fs.existsSync('link_check_results/contributing_results.json')) {
                const contributingResults = JSON.parse(fs.readFileSync('link_check_results/contributing_results.json', 'utf8'));
                issueBody += "\n### CONTRIBUTING.md Broken Links\n\n";
                contributingResults.filter(item => item.status === "error").forEach(item => {
                  issueBody += `- [${item.link}](${item.link}) - ${item.statusCode || 'Unknown error'}\n`;
                });
              }
              
              // Add JSON details if they exist and aren't too many
              if (json_broken > 0 && fs.existsSync('link_check_results/json_results.json')) {
                const jsonResults = JSON.parse(fs.readFileSync('link_check_results/json_results.json', 'utf8'));
                issueBody += "\n### JSON Files Broken Links\n\n";
                
                // Limit to first 30 broken links to avoid issue size limits
                const brokenLinks = jsonResults.details.filter(item => !item.valid).slice(0, 30);
                if (brokenLinks.length > 0) {
                  brokenLinks.forEach(item => {
                    issueBody += `- [${item.url}](${item.url}) - ${item.status || 'Connection error'} - Found in \`${item.path}\`\n`;
                  });
                  
                  if (jsonResults.broken > 30) {
                    issueBody += `\n...and ${jsonResults.broken - 30} more broken links in JSON files. See workflow artifacts for complete list.\n`;
                  }
                }
              }
              
              // Add HTML details if they exist
              if (html_broken > 0 && fs.existsSync('link_check_results/html_results.json')) {
                const htmlResults = JSON.parse(fs.readFileSync('link_check_results/html_results.json', 'utf8'));
                issueBody += "\n### HTML Files Broken Links\n\n";
                
                // Limit to first 30 broken links
                const brokenLinks = htmlResults.failedChecks.slice(0, 30);
                brokenLinks.forEach(check => {
                  issueBody += `- [${check.link}](${check.link}) - ${check.text} - Found in \`${check.filename}\`\n`;
                });
                
                if (htmlResults.failedChecks.length > 30) {
                  issueBody += `\n...and ${htmlResults.failedChecks.length - 30} more broken links in HTML files. See workflow artifacts for complete list.\n`;
                }
              }
            } catch (error) {
              console.error('Error parsing results files:', error);
              issueBody += `\n### Error Reading Detailed Results\n\nThere was an error reading the detailed results: ${error.message}\n`;
            }
            
            // Add instructions
            issueBody += `
            ## How to Fix
            
            1. Review each broken link and either:
               - Update with a working URL
               - Remove the link if no longer relevant
               - Replace with an archived version (e.g., using [Internet Archive](https://web.archive.org/))
            
            2. To see more details, download the workflow artifacts from the Actions tab.
            
            This issue was automatically generated by the Broken Link Checker workflow.
            `;
            
            // Find if there's an existing open issue
            const query = `repo:${context.repo.owner}/${context.repo.repo} is:issue is:open label:broken-links`;
            const issues = await github.rest.search.issuesAndPullRequests({
              q: query
            });
            
            if (issues.data.items.length > 0) {
              // Update existing issue
              const existingIssue = issues.data.items[0];
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: issueBody,
                title: `🔗 ${total_broken} Broken Links Detected - ${new Date().toISOString().split('T')[0]}`
              });
              console.log(`Updated existing issue #${existingIssue.number} with ${total_broken} broken links`);
            } else {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `🔗 ${total_broken} Broken Links Detected - ${new Date().toISOString().split('T')[0]}`,
                body: issueBody,
                labels: ['broken-links', 'documentation']
              });
              console.log(`Created new issue for ${total_broken} broken links`);
            }

      - name: Generate Link Check Report
        if: always()
        run: |
          # Create a markdown report of link check results
          cat > link_check_results/link_report.md << EOF
          # Broken Link Check Report
          
          Generated on: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          
          ## Summary
          
          | Document | Links Checked | Broken Links |
          | -------- | ------------ | ------------ |
          EOF
          
          # Add README stats if available
          if [ -f link_check_results/readme_results.json ]; then
            TOTAL=$(jq '. | length' link_check_results/readme_results.json)
            BROKEN=$(jq '.[] | select(.status == "error") | .link' link_check_results/readme_results.json | wc -l)
            echo "| README.md | $TOTAL | $BROKEN |" >> link_check_results/link_report.md
          else
            echo "| README.md | 0 | 0 |" >> link_check_results/link_report.md
          fi
          
          # Add API stats if available
          if [ -f link_check_results/api_results.json ]; then
            TOTAL=$(jq '. | length' link_check_results/api_results.json)
            BROKEN=$(jq '.[] | select(.status == "error") | .link' link_check_results/api_results.json | wc -l)
            echo "| API.md | $TOTAL | $BROKEN |" >> link_check_results/link_report.md
          else
            echo "| API.md | 0 | 0 |" >> link_check_results/link_report.md
          fi
          
          # Add CONTRIBUTING stats if available
          if [ -f link_check_results/contributing_results.json ]; then
            TOTAL=$(jq '. | length' link_check_results/contributing_results.json)
            BROKEN=$(jq '.[] | select(.status == "error") | .link' link_check_results/contributing_results.json | wc -l)
            echo "| CONTRIBUTING.md | $TOTAL | $BROKEN |" >> link_check_results/link_report.md
          else
            echo "| CONTRIBUTING.md | 0 | 0 |" >> link_check_results/link_report.md
          fi
          
          # Add JSON stats if available
          if [ -f link_check_results/json_results.json ]; then
            TOTAL=$(jq '.checked' link_check_results/json_results.json)
            BROKEN=$(jq '.broken' link_check_results/json_results.json)
            echo "| JSON files | $TOTAL | $BROKEN |" >> link_check_results/link_report.md
          else
            echo "| JSON files | 0 | 0 |" >> link_check_results/link_report.md
          fi
          
          # Add HTML stats if available
          if [ -f link_check_results/html_results.json ]; then
            BROKEN=$(jq '.failedChecks | length' link_check_results/html_results.json)
            TOTAL=$(jq '.htmlDocuments | length' link_check_results/html_results.json)
            echo "| HTML files | $TOTAL | $BROKEN |" >> link_check_results/link_report.md
          else
            echo "| HTML files | 0 | 0 |" >> link_check_results/link_report.md
          fi
          
          # Upload the report as an artifact
          cp link_check_results/link_report.md reports/link_check_report.md || true
      
      - name: Upload Link Check Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: link-check-report
          path: link_check_results/link_report.md